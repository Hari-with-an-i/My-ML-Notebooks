{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Final_DQN_system.ipynb"
      ],
      "metadata": {
        "id": "RyxBkItM4ogk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cof0KkYI1u4j"
      },
      "outputs": [],
      "source": [
        "# Basal Ganglia DQN Simulation\n",
        "# --------------------------------------------\n",
        "# This notebook models the Basal Ganglia neural pathway as a Markov Decision Process (MDP)\n",
        "# and uses a Deep Q-Network (DQN) agent to learn optimal state transitions.\n",
        "# Rewards are modulated by neurochemical levels: Dopamine, Acetylcholine, and Levodopa.\n",
        "\n",
        "import numpy as np  # For numerical operations and random value generation\n",
        "import gym  # OpenAI Gym is used to define and simulate the environment\n",
        "import tensorflow as tf  # TensorFlow is used to build and train the neural network (Q-network)\n",
        "\n",
        "# ----------------------\n",
        "# BasalGangliaMDP Class\n",
        "# ----------------------\n",
        "class BasalGangliaMDP(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom OpenAI Gym environment that models the basal ganglia circuitry using six states\n",
        "    (brain regions), two possible actions (activation/inhibition), and transition probabilities\n",
        "    guided by neurochemical dynamics.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Call the base class constructor (gym.Env)\n",
        "\n",
        "        # Define the key brain regions involved in the basal ganglia loop\n",
        "        self.states = ['Cortex', 'Striatum', 'GPe', 'STN', 'GPi', 'Thalamus']  # 6 states representing brain areas\n",
        "\n",
        "        # Define the two types of actions representing neural activation or inhibition\n",
        "        self.actions = [\"activation\", \"inhibition\"]  # Agent can choose to activate or inhibit\n",
        "        self.action_space = gym.spaces.Discrete(len(self.actions))  # Action space: Discrete(2)\n",
        "\n",
        "        # Define the transitions from one brain region to another based on chosen action\n",
        "        self.transition_probs = {\n",
        "            \"Cortex\": {\n",
        "                \"activation\": {\"Striatum\": 1.0},  # Always go to Striatum if activation chosen\n",
        "                \"inhibition\": {\"Striatum\": 1.0}   # Same for inhibition (simplified model)\n",
        "            },\n",
        "            \"Striatum\": {\n",
        "                \"inhibition\": {'GPe': 0.5, 'GPi': 0.5},  # 50% chance to go to either GPe or GPi\n",
        "            },\n",
        "            \"GPe\": {\n",
        "                \"inhibition\": {\"STN\": 1.0},  # Deterministic transition to STN\n",
        "            },\n",
        "            \"STN\": {\n",
        "                \"activation\": {\"GPi\": 1.0},  # Activation leads to GPi\n",
        "            },\n",
        "            \"GPi\": {\n",
        "                \"inhibition\": {\"Thalamus\": 1.0},  # Inhibition leads to Thalamus\n",
        "            },\n",
        "            \"Thalamus\": {\n",
        "                \"activation\": {\"Cortex\": 1.0},  # Loops back to Cortex\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Define base rewards for certain transitions (will be adjusted dynamically later)\n",
        "        # The reward is given when a specific action leads from one state to another\n",
        "        self.rewards = {\n",
        "            (\"Cortex\", \"activation\", \"Striatum\") : 1.0,\n",
        "            (\"Cortex\", \"inhibition\", \"Striatum\") : 1.0,\n",
        "            (\"Striatum\", \"inhibition\", \"GPe\") : 1.0,\n",
        "            (\"Striatum\", \"inhibition\", \"GPi\") : 1.0,\n",
        "            (\"GPe\", \"inhibition\", \"STN\") : 1.0,\n",
        "            (\"STN\", \"activation\", \"GPi\") : 1.0,\n",
        "            (\"GPi\", \"inhibition\", \"Thalamus\") : 1.0,\n",
        "            (\"Thalamus\", \"activation\", \"Cortex\") : 1.0\n",
        "        }\n",
        "\n",
        "        # Initial state when environment starts\n",
        "        self.state = 'Cortex'\n",
        "\n",
        "    def calculate_rewards(self, state, action, next_state, dopamine, acetyl, levodopa):\n",
        "        \"\"\"\n",
        "        Reward shaping function influenced by neurochemical levels.\n",
        "        Adjusts the base reward based on dopamine, acetylcholine, and levodopa ranges.\n",
        "\n",
        "        Inputs:\n",
        "        - state: current brain region (string)\n",
        "        - action: 'activation' or 'inhibition'\n",
        "        - next_state: resulting brain region\n",
        "        - dopamine, acetyl, levodopa: chemical concentrations affecting behavior\n",
        "\n",
        "        Output:\n",
        "        - numerical reward after applying weight according to condition\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Condition 1: Low dopamine, high acetylcholine, low levodopa\n",
        "            # Represents a simulated Parkinson's state\n",
        "            if(dopamine <= 39.6 and acetyl > 2.5 and levodopa < 100):\n",
        "                # Example: Cortex activates Striatum leads to negative outcome\n",
        "                if(state == \"Cortex\" and next_state == \"Striatum\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * (-100 if action == \"activation\" else 250)\n",
        "                elif(state==\"Striatum\" and next_state==\"GPe\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 400\n",
        "                elif(state==\"Striatum\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -250\n",
        "                elif(state==\"GPe\" and next_state==\"STN\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 500\n",
        "                elif(state==\"STN\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 600\n",
        "                elif(state==\"GPi\" and next_state==\"Thalamus\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 800\n",
        "\n",
        "            # Condition 2: Low dopamine + high acetyl + moderate levodopa (treatment starts)\n",
        "            elif(dopamine <= 39.6 and acetyl > 2.5 and levodopa >= 100 and levodopa <=250):\n",
        "                if(state == \"Cortex\" and next_state == \"Striatum\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * (200 if action == \"activation\" else -150)\n",
        "                elif(state==\"Striatum\" and next_state==\"GPe\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -200\n",
        "                elif(state==\"Striatum\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 450\n",
        "                elif(state==\"GPe\" and next_state==\"STN\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -300\n",
        "                elif(state==\"STN\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -350\n",
        "                elif(state==\"GPi\" and next_state==\"Thalamus\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 1000\n",
        "\n",
        "            # Condition 3: Normal dopamine and acetyl levels\n",
        "            elif(dopamine > 39.6 and dopamine <= 195.8 and acetyl >=0.5 and acetyl <= 2.5):\n",
        "                if(state == \"Cortex\" and next_state == \"Striatum\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * (200 if action == \"activation\" else -150)\n",
        "                elif(state==\"Striatum\" and next_state==\"GPe\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -200\n",
        "                elif(state==\"Striatum\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 550\n",
        "                elif(state==\"GPe\" and next_state==\"STN\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -300\n",
        "                elif(state==\"STN\" and next_state==\"GPi\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * -350\n",
        "                elif(state==\"GPi\" and next_state==\"Thalamus\"):\n",
        "                    return self.rewards.get((state, action, next_state), 0) * 1000\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(\"Error in reward calculation: \" + str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Deep Q-Network Agent\n",
        "# ------------------\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, learning_rate=0.001, discount_factor=0.9, exploration_prob=0.9):\n",
        "        # Dimensions of the state and action spaces\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate              # Learning rate for the optimizer\n",
        "        self.discount_factor = discount_factor          # Gamma: importance of future rewards\n",
        "        self.exploration_prob = exploration_prob        # Epsilon: probability of choosing a random action\n",
        "\n",
        "        # Main Q-network: predicts Q-values for each action\n",
        "        self.q_network = self.build_q_network()\n",
        "\n",
        "        # Target Q-network: used for stable training\n",
        "        self.target_q_network = self.build_q_network()\n",
        "        self.target_q_network.set_weights(self.q_network.get_weights())  # Sync weights initially\n",
        "\n",
        "        # Optimizer for training the model\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "        # Replay memory buffer to store past experiences\n",
        "        self.memory = []\n",
        "\n",
        "    def build_q_network(self):\n",
        "        # Build a simple feed-forward neural network to approximate Q-values\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_space_size,), dtype=tf.float32),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_space_size)  # Output: one Q-value for each action\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Epsilon-greedy strategy: choose a random action with probability epsilon\n",
        "        if np.random.rand() < self.exploration_prob:\n",
        "            return np.random.choice(self.action_space_size)  # Explore\n",
        "        else:\n",
        "            # Convert the state to one-hot encoding\n",
        "            state_one_hot = np.zeros(self.state_space_size)\n",
        "            state_one_hot[state] = 1\n",
        "\n",
        "            # Predict Q-values using the model\n",
        "            q_values = self.q_network.predict(state_one_hot.reshape(1, -1))\n",
        "            return np.argmax(q_values[0])  # Exploit: pick the action with max Q-value\n",
        "\n",
        "    def update_q_network(self, batch_size, states=None):\n",
        "        # Only update if there's enough data in memory\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a batch of experiences from memory\n",
        "        samples = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        batch = [self.memory[i] for i in samples]\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        # Convert state indices to one-hot encoded vectors\n",
        "        states = np.eye(self.state_space_size)[np.array(states)]\n",
        "\n",
        "        # Predict Q-values using both the main and target networks\n",
        "        q_values = self.q_network.predict(states)\n",
        "        next_q_values = self.target_q_network.predict(states)\n",
        "\n",
        "        # Apply the Bellman update equation to each experience in the batch\n",
        "        for i in range(batch_size):\n",
        "            target = rewards[i] + self.discount_factor * np.max(next_q_values[i]) * (1 - dones[i])\n",
        "            q_values[i, actions[i]] = target\n",
        "\n",
        "        # Train the main Q-network with updated Q-values\n",
        "        self.q_network.fit(states, q_values, verbose=0)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        # Copy weights from the main network to the target network\n",
        "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# Generate Inputs (dopamine, acetylcholine, levodopa)\n",
        "# -------------------\n",
        "# These simulate real neurochemical variations for training\n",
        "\n",
        "# Dopamine levels (low and high ranges)\n",
        "dopamine_values = np.concatenate([\n",
        "    np.random.uniform(0, 39.5, 5000),           # Low dopamine range (possibly Parkinsonian)\n",
        "    np.random.uniform(39.5, 195.8, 10000)       # Normal/High range\n",
        "])\n",
        "\n",
        "# Acetylcholine levels (high and normal ranges)\n",
        "acetyl_values = np.concatenate([\n",
        "    np.random.uniform(2.5, 5, 5000),            # Elevated acetylcholine\n",
        "    np.random.uniform(0.5, 2.5, 10000)          # Normal range\n",
        "])\n",
        "\n",
        "# Levodopa levels (low and therapeutic ranges)\n",
        "levodopa_values = np.concatenate([\n",
        "    np.random.uniform(0, 100, 5000),            # No or low Levodopa\n",
        "    np.random.uniform(100, 250, 10000)          # Typical therapeutic range\n",
        "])\n"
      ],
      "metadata": {
        "id": "QUt2BBSb3BtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Train DQN Agent\n",
        "# -------------------\n",
        "\n",
        "# Create an instance of the custom BasalGangliaMDP environment\n",
        "env = BasalGangliaMDP()\n",
        "\n",
        "# Instantiate the Deep Q-Network (DQN) agent\n",
        "agent = DQNAgent(\n",
        "    state_space_size=len(env.states),           # Number of unique brain regions (states)\n",
        "    action_space_size=env.action_space.n        # Number of discrete actions (activation/inhibition)\n",
        ")\n",
        "\n",
        "# Number of episodes (episodes = full runs through a brain loop)\n",
        "num_episodes = 15000\n",
        "\n",
        "# Batch size used for training the neural network using replay buffer\n",
        "batch_size = 32\n",
        "\n",
        "# Loop over all episodes\n",
        "for episode in range(num_episodes):\n",
        "    state_episode = []        # To store the sequence of visited states for context\n",
        "    total_reward = 0          # Accumulator for total reward earned in this episode\n",
        "\n",
        "    # Sample neurochemical levels for this episode\n",
        "    dopamine_value = dopamine_values[episode]\n",
        "    acetyl_value = acetyl_values[episode]\n",
        "    levodopa_value = levodopa_values[episode]\n",
        "\n",
        "    # Reset environment to initial state ('Cortex')\n",
        "    state = env.reset()\n",
        "    state_episode.append(state)  # Track initial state\n",
        "    print(f\"\\nEpisode {episode + 1}:\")\n",
        "\n",
        "    while True:\n",
        "        # Convert the current state name to its index (needed for one-hot encoding)\n",
        "        state_index = env.states.index(state)\n",
        "\n",
        "        # Select an action using epsilon-greedy strategy (explore or exploit)\n",
        "        action = agent.select_action(state_index)\n",
        "\n",
        "        # Some actions (like \"activation\") may not be allowed from certain states.\n",
        "        # Keep re-sampling until a valid action is chosen.\n",
        "        while env.actions[action] not in env.transition_probs[state]:\n",
        "            action = agent.select_action(state_index)\n",
        "\n",
        "        # Perform the action and receive the next state, reward, and done flag\n",
        "        next_state, reward, done, _ = env.step(\n",
        "            env.actions[action],\n",
        "            dopamine_value,\n",
        "            acetyl_value,\n",
        "            levodopa_value\n",
        "        )\n",
        "\n",
        "        # Log the transition in the console\n",
        "        print(f\"Transition: {env.states[state_index]} -> {next_state}, Reward: {reward}\")\n",
        "\n",
        "        # Convert the next state into an index\n",
        "        next_state_index = env.states.index(next_state)\n",
        "\n",
        "        # Store the experience tuple in replay memory\n",
        "        agent.memory.append((state_index, action, reward, next_state_index, done))\n",
        "\n",
        "        # After at least two steps in the episode, update Q-network\n",
        "        if len(state_episode) >= 2:\n",
        "            agent.update_q_network(batch_size, states=state_episode)\n",
        "\n",
        "        # Accumulate the reward\n",
        "        if reward is not None:\n",
        "            total_reward += reward\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # If episode ends (loop reached 'Thalamus'), exit the loop\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Maintain a rolling window of the last 10 visited states for learning context\n",
        "        if len(state_episode) < 10:\n",
        "            state_episode.append(state)\n",
        "        else:\n",
        "            state_episode.pop(0)\n",
        "            state_episode.append(state)\n",
        "\n",
        "    # Every 10 episodes, sync the target network with the Q-network (stabilizes training)\n",
        "    if episode % 10 == 0:\n",
        "        agent.update_target_network()\n",
        "\n",
        "    # Print cumulative reward for this episode\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "nouVAKUr3KQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## arm_N_model_connect.py"
      ],
      "metadata": {
        "id": "0Yf4fsZn5AdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import serial          # For serial communication\n",
        "import time            # For delays\n",
        "import os              # For file path and existence checking\n",
        "import sys             # To exit the program\n",
        "import struct          # (Not used here, can be removed if unnecessary)\n",
        "\n",
        "on = \"1\"  # Default string to write to serial initially (not used in current logic)\n",
        "\n",
        "# ------------------------\n",
        "# Helper function to read values from a file\n",
        "# ------------------------\n",
        "def fileread(line):\n",
        "    \"\"\"\n",
        "    Writes an initial message into a file and waits until a response file is available.\n",
        "    Then, reads comma-separated values from that file and returns them as a list.\n",
        "    \"\"\"\n",
        "    downloads_path = r'C:\\Users\\amrit\\Downloads'\n",
        "\n",
        "    # Create or overwrite a file named \"_init_roboarm.txt\" to indicate startup\n",
        "    file1_path = os.path.join(downloads_path, '_init_roboarm.txt')\n",
        "    with open(file1_path, 'w') as file:\n",
        "        file.write(line)\n",
        "\n",
        "    # Path to the file where the control values are expected\n",
        "    file_path = os.path.join(downloads_path, 'roboarm.txt')\n",
        "\n",
        "    # Wait until this file appears (could be written by another program)\n",
        "    while not os.path.exists(file_path):\n",
        "        time.sleep(3)\n",
        "\n",
        "    # Once file is available, read its contents as comma-separated values\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = [i for i in file.read().split(\",\")]\n",
        "\n",
        "    return content  # Returns list of strings, e.g. ['1.23', '0.45', '-0.67', 'done']\n",
        "\n",
        "# ------------------------\n",
        "# Main Serial Communication Loop\n",
        "# ------------------------\n",
        "try:\n",
        "    # Open the serial port (update 'COM6' if using a different port)\n",
        "    ser = serial.Serial('COM6', 115200)\n",
        "\n",
        "    # Allow time for serial port to properly initialize\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Run loop until task ends or user interrupts\n",
        "    while True:\n",
        "        if ser.is_open:\n",
        "            ser.flush()  # Clears serial buffer to avoid stale data\n",
        "\n",
        "            # Read a line from serial input (from microcontroller)\n",
        "            data = ser.readline()\n",
        "\n",
        "            try:\n",
        "                # Decode the incoming data from bytes to string\n",
        "                line = data.decode('utf-8').strip()\n",
        "                print(\"Received:\", line)\n",
        "\n",
        "                # Check for specific signals from the Arduino/microcontroller\n",
        "                if line == \"Robo arm ready!\":\n",
        "                    # If the robot is ready, send the first command from file\n",
        "                    ser.write(fileread(line)[0].encode())\n",
        "                    print(\"Data Sent\")\n",
        "\n",
        "                elif line == \"2\":\n",
        "                    ser.write(fileread(line)[1].encode())\n",
        "\n",
        "                elif line == \"-0.5528708847336269\":\n",
        "                    ser.write(fileread(line)[2].encode())\n",
        "\n",
        "                elif line == \"0.1485990549217123\":\n",
        "                    ser.write(fileread(line)[3].encode())\n",
        "\n",
        "                elif line == \"Task Ended :(\":\n",
        "                    # Terminate the program gracefully\n",
        "                    print(\"Received 'Task Ended :('. Ending the program.\")\n",
        "                    sys.exit()\n",
        "\n",
        "            except UnicodeDecodeError:\n",
        "                # If decoding fails (corrupt or binary data), print raw bytes\n",
        "                print(\"Received (raw):\", data)\n",
        "\n",
        "        else:\n",
        "            print(\"Serial port is not open.\")\n",
        "            break  # Exit loop if serial port is closed\n",
        "\n",
        "# Handle exceptions gracefully\n",
        "except serial.SerialException as e:\n",
        "    print(\"Serial Error:\", e)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Program terminated by user.\")\n",
        "\n",
        "# Ensure port is closed properly\n",
        "finally:\n",
        "    if ser.is_open:\n",
        "        ser.close()\n"
      ],
      "metadata": {
        "id": "dLfUMbWF5AH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  Project Overview\n",
        "\n",
        "This project simulates the **Basal Ganglia neural circuitry** using a **Markov Decision Process (MDP)**, trains a **Deep Q-Network (DQN)** agent to learn optimal neural transitions, and eventually **interacts with an Arduino-controlled robotic arm** based on learned behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## MODULE 1: Basal Ganglia MDP Simulation\n",
        "\n",
        "### What it models:\n",
        "\n",
        "The **BasalGangliaMDP** class defines a simplified brain model with 6 neural regions and transition paths modulated by actions (`activation`, `inhibition`) and **neurochemical signals**.\n",
        "\n",
        "### States:\n",
        "\n",
        "* `'Cortex'`\n",
        "* `'Striatum'`\n",
        "* `'GPe'` (Globus Pallidus externa)\n",
        "* `'STN'` (Subthalamic Nucleus)\n",
        "* `'GPi'` (Globus Pallidus interna)\n",
        "* `'Thalamus'`\n",
        "\n",
        "### Actions:\n",
        "\n",
        "* `\"activation\"`\n",
        "* `\"inhibition\"`\n",
        "\n",
        "### Transitions:\n",
        "\n",
        "Defined probabilistically (some deterministic), e.g.,\n",
        "\n",
        "* Cortex → Striatum on any action.\n",
        "* Striatum → GPe/GPi (inhibition, 50-50 chance).\n",
        "* GPi → Thalamus (inhibition).\n",
        "* Thalamus → Cortex (activation, closes the loop).\n",
        "\n",
        "### Neurochemical Reward Modulation:\n",
        "\n",
        "The reward for each transition is adjusted based on the simulated levels of:\n",
        "\n",
        "* **Dopamine** (low → Parkinsonian)\n",
        "* **Acetylcholine**\n",
        "* **Levodopa** (treatment)\n",
        "\n",
        "> For example, low dopamine + high acetylcholine gives **very negative rewards** for \"activation\" of Cortex → Striatum.\n",
        "\n",
        "---\n",
        "\n",
        "## MODULE 2: Deep Q-Network Agent (DQN)\n",
        "\n",
        "### DQNAgent Class:\n",
        "\n",
        "* Uses a simple neural network to estimate Q-values.\n",
        "* Supports:\n",
        "\n",
        "  * **Epsilon-greedy exploration**\n",
        "  * **Replay memory**\n",
        "  * **Target network for stability**\n",
        "  * **One-hot encoded states**\n",
        "\n",
        "### Neural Net Architecture:\n",
        "\n",
        "```text\n",
        "Input: One-hot encoded state (length 6)\n",
        "Hidden: 64 → 64 ReLU\n",
        "Output: Q-values for each of the 2 actions\n",
        "```\n",
        "\n",
        "### Training:\n",
        "\n",
        "* Stores transitions `(state, action, reward, next_state, done)` in memory.\n",
        "* Periodically samples a batch and updates Q-network using **Bellman equation**.\n",
        "\n",
        "---\n",
        "\n",
        "## MODULE 3: Training Loop\n",
        "\n",
        "### Inputs Simulated:\n",
        "\n",
        "* 15,000 episodes, each with:\n",
        "\n",
        "  * A random dopamine, acetylcholine, and levodopa level (sampled from predefined distributions).\n",
        "\n",
        "### Training Flow:\n",
        "\n",
        "1. **Reset** to `'Cortex'`.\n",
        "2. **Loop** until reaching `'Thalamus'`:\n",
        "\n",
        "   * Choose action using `select_action()`.\n",
        "   * Perform transition using `env.step()`.\n",
        "   * Get neurochemically-modulated reward.\n",
        "   * Store experience and update Q-network (if enough history).\n",
        "3. Every 10 episodes: sync target network.\n",
        "4. Print total reward at end of episode.\n",
        "\n",
        "---\n",
        "\n",
        "## MODULE 4: Serial Communication with Robo Arm\n",
        "\n",
        "### Goal:\n",
        "\n",
        "This part listens to a **serial port (COM6)** connected to a **microcontroller/Arduino**, receives signals, and sends back control commands based on pre-generated outputs.\n",
        "\n",
        "### File Interaction:\n",
        "\n",
        "* Writes `\"Robo arm ready!\"` status to `\"_init_roboarm.txt\"` in `Downloads`.\n",
        "* Waits for `roboarm.txt` to appear in Downloads.\n",
        "* Reads values from `roboarm.txt`, expecting 4 comma-separated float values (possibly robotic joint angles or actions).\n",
        "* Sends specific values back to the robot when it receives matching signals.\n",
        "\n",
        "### Flow:\n",
        "\n",
        "```text\n",
        "1. Waits for serial data (e.g., \"Robo arm ready!\", \"2\", etc.)\n",
        "2. Matches received signal to a value index.\n",
        "3. Reads value from roboarm.txt and sends it over serial.\n",
        "4. Terminates cleanly on \"Task Ended :(\" signal.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Full Flow Diagram Summary\n",
        "\n",
        "```text\n",
        "[Dopamine/Acetyl/Levodopa Inputs] ───────┐\n",
        "                                        ▼\n",
        "  [BasalGangliaMDP Environment] ──> [Reward Calculation]\n",
        "                                        │\n",
        "                                        ▼\n",
        "  [DQN Agent] ──────> [Action Selection (ε-greedy)]\n",
        "                          │\n",
        "                          ▼\n",
        "  [State Transition + Experience Storage] ──> [Q-Network Update (Batch)]\n",
        "                          │\n",
        "                          ▼\n",
        "                    Repeat till done (reaches Thalamus)\n",
        "                                        │\n",
        "                                        ▼\n",
        "                Trained model informs control values\n",
        "                                        │\n",
        "                                        ▼\n",
        "             [File: roboarm.txt] ←──── [Serial Port Reads]\n",
        "                                        │\n",
        "                                        ▼\n",
        "               [Arduino Receives and Acts on Data]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## What’s Unique About This Project?\n",
        "\n",
        "* Combines **neuroscience modeling** with **reinforcement learning**.\n",
        "* Adjusts learning behavior based on **biological chemical concentrations**.\n",
        "* Interfaces with **hardware (robot arm)** via **serial communication**.\n",
        "* Helps simulate disorders like **Parkinson’s Disease** and see how medication (Levodopa) affects neural decision-making paths.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TGJEmh79-v3_"
      }
    }
  ]
}